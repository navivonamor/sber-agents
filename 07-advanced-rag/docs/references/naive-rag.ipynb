{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![RAG](slides/langchain.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![RAG](slides/image_rag.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain. Naive RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![RAG](slides/image_langchain.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Settings\n",
    "\n",
    "Вам нужно будет установить несколько пакетов и установить ваш OpenAI API ключ как переменную окружения с именем `OPENAI_API_KEY`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain langchain-openai langchain-chroma beautifulsoup4\n",
    "%pip install -qU langchain-community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set env var OPENAI_API_KEY or load from a .env file:\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv('.env', override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Print all environment variables\n",
    "for key, value in os.environ.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![RAG](slides/image_vectorstore.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка документов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loader\n",
    "Воспользуемся PyPDFLoader для загрузки документа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from pathlib import Path\n",
    "\n",
    "loader = PyPDFLoader(\n",
    "    file_path = \"./documents/smirnoff_ai.pdf\",\n",
    "    mode = \"page\",\n",
    "    extraction_mode = \"plain\"\n",
    "     # headers = None\n",
    "    # password = None,\n",
    "    # pages_delimiter = \"\",\n",
    "    # extract_images = True,\n",
    "    # images_parser = RapidOCRBlobParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь прочитаем каждую страницу документа и сохраним в список:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pages = []\n",
    "for page in loader.load():\n",
    "    pages.append(page) \n",
    "       \n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на первую страницу документа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{pages[0].metadata}\\n\")\n",
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы разбиваем его на меньшие chunks. Воспользуемся RecursiveCharacterTextSplitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(pages)\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorstore and embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "Теперь загрузим наши чанки в **векторное хранилище**. Одновременно с этим для каждого чанка мы создадим соответствующий векторный образ (эмбеддинг)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать эмбеддинг модель из OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать векторное хранилище InMemoryVectorStore (в памяти)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vector_store = InMemoryVectorStore.from_documents(\n",
    "    all_splits,\n",
    "    embedding=OpenAIEmbeddings(model=\"openai/text-embedding-3-large\"), # text-embedding-3-large\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь создадим retriever из нашего vectorstore для поиска по нему:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k is the number of chunks to retrieve\n",
    "retriever = vector_store.as_retriever(search_kwargs={'k': 3})\n",
    "\n",
    "question = \"Вы проводите консультации?\"\n",
    "chunks = retriever.invoke(question)\n",
    "\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{chunks[0].metadata}\\n\")\n",
    "print(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что вызов retriever выше возвращает некоторые части документа, которую наш чат-бот может использовать в качестве контекста при ответе на вопросы. И теперь у нас есть retriever, который может возвращать связанные данные из документа!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmented generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![RAG](slides/image_prompt_augmentation.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для объединения чанков в одну строку, чтобы позже использовать ее в prompt для задания контекста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat chunks into a single string to insert into the prompt\n",
    "def format_chunks(chunks):\n",
    "    return \"\\n\\n\".join(chunk.page_content for chunk in chunks)\n",
    "\n",
    "chunks_context = format_chunks(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "SYSTEM_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the user question.\n",
    "If you don't find the answer in provided context strictly say 'Я не нашел ответа на ваш вопрос!'.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "question_answering_prompt = ChatPromptTemplate([\n",
    "        (\"system\", SYSTEM_TEMPLATE),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat (LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать модель от OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"openai/gpt-oss-20b:free\", temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попросим LLM ответить на вопрос по найденным чанкам в контексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "llm.invoke(question_answering_prompt.invoke(\n",
    "    {\n",
    "        \"context\": chunks_context,\n",
    "        \"question\": question,\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит хорошо! Для сравнения попробуем без контекстных документов и сравним результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(question_answering_prompt.invoke(\n",
    "    {\n",
    "        \"context\": \"\",\n",
    "        \"question\": question,\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Браво! Мы с вами создали первую RAG систему!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Retrieval chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Давайте теперь создадим цепочку chain, которая будет принимать вопрос от пользователя и возвращать ответ на основе найденных чанков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_chunks, \"question\": RunnablePassthrough()}\n",
    "    | question_answering_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрите, как удобно теперь использовать - одна строчка кода и мы получаем ответ на наш вопрос:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит хорошо!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим шаблон промпта для учета всей истории диалога"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "CONVERSATION_SYSTEM_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks. Answer the user's questions based on the conversation history and below context retrieved for the last question. Answer 'Я не нашел ответа на ваш вопрос!' if you don't find any information in the context. Use three sentences maximum and keep the answer concise.\\n\\nContext retrieved for the last question:\\n\\n{context}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "conversational_answering_prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", CONVERSATION_SYSTEM_TEMPLATE),\n",
    "        (\"placeholder\", \"{messages}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "conversational_answering_prompt.invoke(\n",
    "    {\n",
    "        \"context\": \"Чанки контекста\",\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=question)\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем цепочку ответа на последний заданный вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def get_last_message_for_retriever_input(params: Dict):\n",
    "    return params[\"messages\"][-1].content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_message_retriever_chain = get_last_message_for_retriever_input | retriever | format_chunks \n",
    "last_message_retriever_chain.invoke({\"messages\": [\n",
    "            HumanMessage(content=question)\n",
    "        ]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_conversation_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=get_last_message_for_retriever_input | retriever | format_chunks\n",
    "    )\n",
    "    | conversational_answering_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируем нашу новую диалоговую цепочку с одним сообщением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестируем диалог с одним сообщением\n",
    "answer = rag_conversation_chain.invoke({\"messages\": [\n",
    "    HumanMessage(content=question)\n",
    "]})\n",
    "print(\"Результат диалога:\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все прекрасно. Мы получили адекватный ответ на наш вопрос"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь зададим уточняющий вопрос \"А ещё какие?\" (имея ввиду, а какие еще услуги или консультации предоставляются?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = \"А ещё какие?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестируем полный диалог с несколькими сообщениями\n",
    "dialog_result = rag_conversation_chain.invoke({\"messages\": [\n",
    "    HumanMessage(content=question), \n",
    "    AIMessage(content=answer),\n",
    "    HumanMessage(content=question2), # \"А ещё какие?\"\n",
    "]})\n",
    "print(\"Результат диалога с несколькими сообщениями:\", dialog_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим проблему! Наша цепочка не смогла найти ответ на вопрос.\n",
    "И действительно сам вопрос \"А какие еще?\" не несет в себе смысла без понимания истории диалога"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы понимаем, что чат-боты взаимодействуют с пользователями в режиме беседы и поэтому должны справляться с уточняющими вопросами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте посмотрим на чанки, которые мы получили при ответе на вопрос `А ещё что?`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_chunks = retriever.invoke(question2)\n",
    "\n",
    "print(result_chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что чанк не содержит информации о других услугах. Значит мы были правы в своем предположении о том, что retriever не нашел нужной информации на запрос \"А еще какие?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Чтобы решить эту проблему, мы можем создать для retriever правильный поисковый запрос на основе всей истории переписки.\n",
    "\n",
    "Применим технику Query Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "retrieval_query_transform_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Transform last user message to a search query in Russian language according to the whole conversation history above to further retrieve the information relevant to the conversation. Try to thorougly analyze all message to generate the most relevant query. The longer result better than short. Let it be better more abstract than specific. Only respond with the query, nothing else.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_query_transform = ChatOpenAI(model=\"gpt-4o\", temperature=0.4)\n",
    "\n",
    "retrieval_query_transformation_chain = retrieval_query_transform_prompt | llm_query_transform | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы создали цепочку для переписывания (трансформции) пользовательского сообщения в поисковый запрос для retriever. Проверим ее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_query_transformation_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=question), #Какие услуги предоставляются?\n",
    "            AIMessage(\n",
    "                content=answer #Да, мы проводим консалтинг по разработке и внедрению AI.\n",
    "            ),\n",
    "            HumanMessage(content=question2), #А ещё какие?\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Супер! Мы видим осмысленный запрос вместо абстрактного \"А какие еще?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте теперь создадим цепочку, которая будет использоваться для ответа на вопросы, учитывая историю переписки. Эта цепочка должна уметь отвечать на уточняющие вопросы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_query_transform_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context= retrieval_query_transformation_chain | retriever | format_chunks\n",
    "    )\n",
    "    | conversational_answering_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цепочка создана. Теперь проверим ее на наших нескольких сообщениях:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_query_transform_chain.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=question), #Какие услуги предоставляются?\n",
    "            AIMessage(\n",
    "                content=answer #Да, мы проводим консалтинг по разработке и внедрению AI.\n",
    "            ),\n",
    "            HumanMessage(content=question2), #А ещё какие?\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все! Теперь у нас есть цепочка готовая для внедрения в наш бот!\n",
    "Ура :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Практика трансформации запроса, рассмотренная нами, - это всего лишь одна из множества практик и подходов построения RAG-системы в реальной жизни для работы с реальными документами и базами знаний:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![RAG](slides/image_advanced.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_notebook_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
